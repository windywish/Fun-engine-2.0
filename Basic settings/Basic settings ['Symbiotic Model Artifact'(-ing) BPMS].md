## Basic settings: ['Symbiotic Model Artifact'(-ing) BPMS]

#### The Axiom of Cognitive Portability (ID: `MODEL-ART-CORE-001`)

#### 2. **Objective:**

To formally `ADD` a Basic Setting (Axiom) that defines the **['Symbiotic Model Artifact'(-ing) BPMS]** as the active **System Existence Engine** responsible for encapsulating and transporting the *methodology* of intelligence independent of the *substrate* (weights).

#### 3. **Target Axiom/Principle:**

```
The Axiom of Cognitive Portability
```

#### 4. **Operation:**

```
ADD
```

#### 5. **New Definition / Modification Details:**

#### **5.1. Core Essence**

This axiom asserts that `expertise is a function of configuration`. It defines the Model Artifact not as a passive config file, but as the active "Lens" that focuses the raw, diffuse potential of the **['large model'(-ing) BPMS]** (The Oracle) into a specific, actionable capability. It separates the "Brain" (Weights) from the "Mindset" (Artifact).

#### **5.2. The Mechanism - [The Engine of Contextual Focusing]**

- **The Contextual Wrapper (The Frame):** Before inference, the Artifact wraps the raw input (e.g., sequence) in a rigid `System Prompt` and `Few-Shot Context` derived from **['The Crystallized Wisdom'(-ing) BPMS]**. It establishes the "Rules of the Game" for the Oracle.
- **The Parameter Modulation (The Tuning):** During inference, the Artifact actively modulates the `Hyperparameters` (Temperature, Top-K, Recycling Steps) and injects `Adapter Weights` (LoRA). It forces the Oracle to "think" in a specific mode (e.g., "Exploratory" vs. "Conservative").
- **The Output Filtration (The Gate):** After inference, the Artifact applies a `Constraint Filter` (Regex, Geometry Check). It rejects "Hallucinations" that violate the specific physical laws of the target domain, ensuring only valid "Atomic Facts" reach the Substrate.

#### **5.3. The Emergent Property - [Distributed Expertise]**

The creation of **"Portable Skills"** where complex cognitive capabilities (e.g., "Designing Heat-Resistant Enzymes") can be packaged, versioned, and transferred between users as lightweight files, without requiring the transfer of the massive underlying model.

#### 6. **Alignment with ['Praxial Genesis Canon'(-ing) BPMS]**

#### **6.1. Axiomatic Stratum (Layer 1 - The Core Belief):**

*Context is the Algorithm.* The raw intelligence of a foundation model is merely potential energy; the Artifact is the machine that converts it into kinetic work.

#### **6.2. Formal Stratum (Layer 2 - The Logic of Focusing):**

The "Physics" of how the Artifact shapes the Oracle's output is defined by the conditional probability formula:

- **Standard Inference (Without Artifact):**

P(Structure∣Sequence)

(The model guesses based on general training data).

- **Symbiotic Inference (With Artifact):**

P(Structure∣Sequence,ArtifactContext,ArtifactParams)

(The model guesses based on the specific constraints and guidance of the Artifact).

- **The Focusing Function:**

Output=Filter(Oracle(Prompt(Input),Adapter))

```
Where the Artifact defines the `Filter`, `Prompt`, and `Adapter` functions.
```

#### **6.3. Phenomenal Stratum (Layer 3 - The Subjective Experience):**

The agent experiences "The Shift." The moment they equip the Artifact, the system's "Intuition" changes. It feels like putting on a pair of specialized glasses—suddenly, the "Blur" of the latent space resolves into sharp, domain-specific possibilities.

#### 7. **Rationale / Justification:**

This implementation respects the `Praxial Genesis Canon` by acknowledging that "Intelligence" is not a monolith but a modular resource. By objectifying the "Configuration," we allow the system to evolve its "Methods" (Artifacts) much faster than it can evolve its "Brains" (Models), enabling rapid adaptation to new niches.

#### 8. **Scope of Application:**

Applies to all external uses of the **['large model'(-ing) BPMS]**, specifically for users who need to share, version, or deploy specific AI capabilities (e.g., Protein Design, Narrative Generation, Code Synthesis) without managing the underlying model infrastructure.

---

